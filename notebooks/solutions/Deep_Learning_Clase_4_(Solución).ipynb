{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RodolfoFerro/dl-facilito-g2/blob/main/notebooks/solutions/Deep_Learning_Clase_4_(Soluci%C3%B3n).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6jO_1gISKxk"
      },
      "source": [
        "# Deep Learning - Clase 4  üß†\n",
        "\n",
        "> **Descripci√≥n:** Cuaderno de contenidos (IV) sobre introducci√≥n a _deep learning_ para el Bootcamp en DS con C√≥digo Facilito, 2023. <br>\n",
        "> **Autor:** [Rodolfo Ferro](https://github.com/RodolfoFerro) <br>\n",
        "> **Contacto:** [Twitter](https://twitter.com/rodo_ferro) / [Instagram](https://www.instagram.com/rodo_ferro/) \n",
        "\n",
        "\n",
        "## Contenido\n",
        "\n",
        "### Pre√°mbulo\n",
        "\n",
        "- Introducci√≥n a la reconstrucci√≥n de im√°genes\n",
        "\n",
        "### Secci√≥n X\n",
        "\n",
        "33. Introducci√≥n a los autoencoders\n",
        "34. Estructura b√°sica de un autoencoder\n",
        "35. Funcionamiento de un autoencoder\n",
        "\n",
        "\n",
        "### Secci√≥n XI\n",
        "\n",
        "36. Limitaciones de los autoencoders b√°sicos\n",
        "37. Introducci√≥n a los autoencoders convolucionales\n",
        "38. Estructura de los autoencoders convolucionales\n",
        "\n",
        "\n",
        "\n",
        "### Secci√≥n XII ‚Äì Tarea\n",
        "\n",
        "30. Implementaci√≥n y entrenamiento de los autoencoders convolucionales\n",
        "31. Aplicaciones de los autoencoders convolucionales en el denoising de im√°genes\n",
        "32. Trabajos relacionados y avances recientes\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNVG2PnSEtQN"
      },
      "source": [
        "## **Pre√°mbulo**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introducci√≥n a la reconstrucci√≥n de im√°genes\n",
        "\n",
        "\n",
        "- Proceso de generar una imagen de salida a partir de una de entrada, generalmente con el objetivo de **restaurar o mejorar la calidad de la imagen original**.\n",
        "- En el contexto de los autoencoders y la tarea de denoising, la reconstrucci√≥n implica generar una versi√≥n limpia y libre de ruido de una imagen ruidosa o de baja calidad.\n",
        "\n",
        "Es √∫til y ayuda para:\n",
        "- Restauraci√≥n de la calidad visual.\n",
        "- Preservaci√≥n de la informaci√≥n relevante.\n",
        "- Mejora de aplicaciones y an√°lisis (eliminaci√≥n de ruido).\n",
        "- Preservaci√≥n de caracter√≠sticas y texturas.\n",
        "- Calidad y experiencia visual.\n",
        "\n",
        "<center>\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/0*ZE8YXDof_MVxk43k.png\" width=\"50%\">\n",
        "</center>"
      ],
      "metadata": {
        "id": "IOOWWpUv32iz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VS_zSehl30t1"
      },
      "source": [
        "## **Secci√≥n X**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introducci√≥n a los autoencoders\n",
        "\n",
        "Son ANNs utilizadas para aprender representaciones eficientes de los datos de entrada sin necesidad de etiquetas o supervisi√≥n externa."
      ],
      "metadata": {
        "id": "2k4ZBY7drHpQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Estructura b√°sica y funcionamiento de un autoencoder\n",
        "\n",
        "Est√°n dise√±adas para aprender representaciones eficientes de los datos de entrada mediante un proceso de compresi√≥n y descompresi√≥n.\n",
        "\n",
        "Las partes fundamentales son el encoder y el decoder.\n",
        "\n",
        "<center>\n",
        "    <img src=\"https://lilianweng.github.io/posts/2018-08-12-vae/autoencoder-architecture.png\" width=\"70%\">\n",
        "</center>\n",
        "\n",
        "#### Encoder\n",
        "\n",
        "- **Encoder:** Toma los datos de entrada y los transforma en una representaci√≥n de menor dimensionalidad, tambi√©n conocida como representaci√≥n latente. \n",
        "- **El objetivo del codificador es‚Ä¶** Comprimir la informaci√≥n esencial de los datos en esta representaci√≥n latente. \n",
        "- **Usualmente**, el codificador est√° compuesto por capas de neuronas que reducen gradualmente la dimensionalidad de los datos a medida que se propagan a trav√©s de la red. Este proceso de reducci√≥n dimensional es crucial para extraer las caracter√≠sticas m√°s importantes de los datos y deshacerse de la informaci√≥n redundante o ruidosa.\n",
        "\n",
        "#### Decoder\n",
        "\n",
        "- **Decoder:** Toma la representaci√≥n latente generada y la reconstruye en una salida que se asemeja lo m√°s posible a la entrada original. \n",
        "- **El objetivo del decodificador es‚Ä¶** Descomprimir la representaci√≥n latente y generar una reconstrucci√≥n fiel de los datos de entrada.\n",
        "- **Al igual que el encoder**, el decodificador est√° compuesto por capas de neuronas, pero en este caso, las capas aumentan gradualmente la dimensionalidad de la representaci√≥n latente hasta que se obtiene una salida de la misma dimensi√≥n que los datos de entrada originales.\n",
        "\n",
        "**La idea central es que**, al restringir la capacidad de reconstrucci√≥n de la red, se obliga al codificador a aprender representaciones m√°s compactas y significativas de los datos. \n",
        "\n",
        "**En otras palabras**, se busca que el codificador capture las caracter√≠sticas m√°s importantes y relevantes de los datos en la representaci√≥n latente, mientras que el decodificador se encarga de reconstruir los datos de entrada a partir de esa representaci√≥n latente. \n",
        "\n",
        "#### Aplicaicones de autoencoders\n",
        "\n",
        "- **Denoising de im√°genes:** Se utilizan para eliminar el ruido de las im√°genes y reconstruir versiones m√°s limpias.\n",
        "Generaci√≥n de contenido: Pueden generar contenido nuevo y original aprendiendo las caracter√≠sticas latentes de un conjunto de datos. Esto se utiliza en aplicaciones como la generaci√≥n de im√°genes sint√©ticas, la creaci√≥n de m√∫sica o la generaci√≥n de texto.\n",
        "- **Detecci√≥n de anomal√≠as:** Se utilizan para detectar patrones anormales o inusuales en los datos. Esto se aplica en √°reas como la detecci√≥n de fraudes en transacciones financieras, la detecci√≥n de intrusiones en sistemas de seguridad o la detecci√≥n de anomal√≠as en im√°genes m√©dicas.\n",
        "- **Reducci√≥n de dimensionalidad:** Los autoencoders se utilizan para reducir la dimensionalidad de los datos, lo que facilita la visualizaci√≥n y comprensi√≥n de datos complejos. Esto es √∫til en an√°lisis exploratorio de datos, visualizaci√≥n de datos de gran dimensi√≥n y clustering.\n",
        "\n",
        "#### Aprendizaje\n",
        "\n",
        "- **Aprendizaje:** Durante el entrenamiento de los autoencoders, se utilizan pares de datos de entrada y salida correspondientes. \n",
        "- La red se entrena para **minimizar** la diferencia entre la entrada original y la salida reconstruida.."
      ],
      "metadata": {
        "id": "5dy-RQOw4kQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "1kb3YuuW51-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos los datos."
      ],
      "metadata": {
        "id": "1RbMybth8xL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, _), (x_test, _) = fashion_mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "print (x_train.shape)\n",
        "print (x_test.shape)"
      ],
      "metadata": {
        "id": "58k-OiKs7WQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(x_train[0], cmap='gray')"
      ],
      "metadata": {
        "id": "Hr5CjhCK-Asn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos la clase Autoencoder con TensorFlow."
      ],
      "metadata": {
        "id": "_9VwrJXJ8YSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder(Model):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.latent_dim = latent_dim   \n",
        "\n",
        "        # Construimos el encoder\n",
        "        self.encoder = tf.keras.Sequential([\n",
        "\n",
        "            # TODO: A√±ade una capa Flatten\n",
        "            layers.Flatten(),\n",
        "            \n",
        "            # TODO: A√±ade una capa Dense -> latent_dim, ReLU\n",
        "            layers.Dense(latent_dim, activation='relu'),\n",
        "\n",
        "        ])\n",
        "\n",
        "        # Construimos el decoder\n",
        "        self.decoder = tf.keras.Sequential([\n",
        "            \n",
        "            # TODO: A√±ade una capa Dense -> 784, Sigmoid\n",
        "            layers.Dense(784, activation='sigmoid'),\n",
        "            \n",
        "            # TODO: A√±ade una capa Reshape -> (28, 28)\n",
        "            layers.Reshape((28, 28))\n",
        "\n",
        "        ])\n",
        "\n",
        "    def call(self, x):\n",
        "        # Esta funci√≥n nos permite invocar los sub modelos\n",
        "        # para poder hacer inferencia sobre cada uno (predict)\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        \n",
        "        return decoded"
      ],
      "metadata": {
        "id": "1qeFNIHw7i4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos par√°metros y creamos un autoencoder."
      ],
      "metadata": {
        "id": "Kx5QY8uY8qSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos dimensi√≥n de espacio latente\n",
        "latent_dim = 64\n",
        "\n",
        "# Instanciamos un autoencoder\n",
        "autoencoder = Autoencoder(latent_dim)"
      ],
      "metadata": {
        "id": "ZO8FSy2S7pXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compilamos y entrenamos."
      ],
      "metadata": {
        "id": "GjfNJhov8tYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "history = autoencoder.fit(x_train, x_train,\n",
        "                          epochs=10,\n",
        "                          shuffle=True,\n",
        "                          validation_data=(x_test, x_test))"
      ],
      "metadata": {
        "id": "Si_2tc2u8is5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.summary()"
      ],
      "metadata": {
        "id": "mZexUsya-dwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-v0_8')\n",
        "\n",
        "x = np.arange(len(history.history['loss']))\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(x, loss, label='Training')\n",
        "plt.plot(x, val_loss, label='Validation')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.savefig('loss_autoencoder.png', dpi=300)"
      ],
      "metadata": {
        "id": "YWm7Pi2Q-fdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploramos los resultados."
      ],
      "metadata": {
        "id": "sUdWJBEO8vC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_imgs = autoencoder.encoder(x_test).numpy()\n",
        "decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()"
      ],
      "metadata": {
        "id": "Y8ZfJ8J__j5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 10\n",
        "plt.figure(figsize=(20, 4))\n",
        "\n",
        "for i in range(n):\n",
        "    # Mostramos imagen original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(x_test[i + 121])\n",
        "    plt.title(\"Original\")\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Mostramos imagen reconstruida\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(decoded_imgs[i + 121])\n",
        "    plt.title(\"Reconstruida\")\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DF9NFmQ-7q_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reto:** ¬øPuedes mejorar a√∫n m√°s las reconstrucciones?\n",
        "\n",
        "Te recomiendo explorar lo siguiente:\n",
        "- Modifica el n√∫mero de capas y neuronas por capa. Recuerda que debe ir disminuyendo en el encoder y aumentando en el decoder y deben tener una estructura espejeada.\n",
        "- Modifica el n√∫mero de √©pocas de entrenamiento.\n",
        "- ¬øQuieres intentar reconstruir cosas con convoluciones? Te invito a que lo intentes.\n",
        "\n",
        "\n",
        "**Lecturas recomendadas:**\n",
        "- [A 2021 Guide to improving CNNs-Optimizers: Adam vs SGD](https://medium.com/geekculture/a-2021-guide-to-improving-cnns-optimizers-adam-vs-sgd-495848ac6008)"
      ],
      "metadata": {
        "id": "7r1Tx5AyBAK3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rtpuj0PBqWYc"
      },
      "source": [
        "## **Secci√≥n XI**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Limitaciones de autoencoders b√°sicos\n",
        "\n",
        "- **Incapacidad para capturar informaci√≥n espacial:** Los autoencoders b√°sicos pueden tener dificultades para capturar la estructura espacial de las im√°genes, ya que no tienen en cuenta la informaci√≥n de vecindad de los p√≠xeles.\n",
        "- **Sensibilidad a las transformaciones:** Pueden ser sensibles a las transformaciones geom√©tricas, como la rotaci√≥n o el desplazamiento de la imagen, lo que puede afectar su capacidad de reconstrucci√≥n.\n",
        "- **Autoencoders convolucionales como soluci√≥n:** Son una variante de los autoencoders que incorporan capas convolucionales para abordar las limitaciones mencionadas.\n",
        "- **Ventajas de los autoencoders convolucionales:** Mejoran con la capacidad para capturar algunas caracter√≠sticas espaciales, preservar la estructura y ser m√°s robustos frente a transformaciones geom√©tricas\n",
        "\n"
      ],
      "metadata": {
        "id": "jJdHo17RAp4S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Autoencoders convolucionales\n",
        "\n",
        "La estructura de un autoencoder convolucional consta de dos partes principales: el codificador (encoder) y el decodificador (decoder). Cada una de estas partes est√° compuesta por capas convolucionales, capas de muestreo y, en algunos casos, capas de convoluci√≥n transpuesta o de upsampling. \n",
        "\n",
        "<center>\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*TOJD69Y8dZsKFEW-21xUPg.png\" width=\"70%\">\n",
        "</center>\n",
        "\n",
        "#### Encoder\n",
        "\n",
        "- **Capas convolucionales:** Estas capas utilizan filtros convolucionales para extraer caracter√≠sticas de nivel superior de la entrada.\n",
        "- **Capas de pooling:** Estas capas reducen la dimensionalidad de las caracter√≠sticas extra√≠das al realizar un muestreo o reducci√≥n de tama√±o, como el max pooling.\n",
        "- **Funciones de activaci√≥n:** Despu√©s de cada capa convolucional y de muestreo, se aplica una funci√≥n de activaci√≥n no lineal, como la funci√≥n ReLU (Rectified Linear Unit), para introducir no linealidad en la red.\n",
        "\n",
        "#### Espacio latente\n",
        "\n",
        "- **Capa de aplanamiento:** Antes de llegar al espacio latente, la salida del codificador se aplanar√° en un vector unidimensional.\n",
        "- **Capa densa (fully connected):** La capa densa o fully connected reduce a√∫n m√°s la dimensionalidad y mapea las caracter√≠sticas a un espacio latente de menor dimensi√≥n. Esta capa suele tener una funci√≥n de activaci√≥n, como la ReLU o la tangente hiperb√≥lica.\n",
        "\n",
        "#### Decoder\n",
        "\n",
        "- **Capas densas (fully connected):** En el decodificador, se utilizan capas densas para aumentar gradualmente la dimensionalidad del espacio latente y reconstruir las caracter√≠sticas originales.\n",
        "- **Capas de convoluci√≥n transpuesta o upsampling:** Estas capas realizan la operaci√≥n inversa de las capas de muestreo, aumentando gradualmente el tama√±o espacial de las caracter√≠sticas.\n",
        "- **Capas de convoluci√≥n:** Al final del decodificador, se utilizan capas convolucionales para generar una salida final con las mismas dimensiones que la entrada original.\n",
        "- **Funci√≥n de activaci√≥n final:** La funci√≥n de activaci√≥n final depende del rango de valores de la imagen de salida. Por ejemplo, en im√°genes en escala de grises, se puede utilizar una funci√≥n de activaci√≥n sigmoide para obtener valores entre 0 y 1.\n",
        "\n",
        "La estructura del autoencoder convolucional puede variar seg√∫n la tarea espec√≠fica y los requisitos del problema. Se pueden agregar capas adicionales, como capas de regularizaci√≥n, capas de normalizaci√≥n o capas de convoluci√≥n dilatadas, para mejorar el rendimiento y la capacidad de generalizaci√≥n del modelo."
      ],
      "metadata": {
        "id": "Vs8JZj5D6Jgx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KKqdPJBCU_E"
      },
      "source": [
        "## **Secci√≥n XII**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Implementaci√≥n y entrenamiento de los autoencoders convolucionales"
      ],
      "metadata": {
        "id": "WCO2eFvsCOVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Denoise(Model):\n",
        "    def __init__(self):\n",
        "        super(Denoise, self).__init__()\n",
        "\n",
        "        # Creamos el encoder convolucional\n",
        "        self.encoder = tf.keras.Sequential([\n",
        "\n",
        "            # TODO: A√±ade una capa Dense -> latent_dim, ReLU\n",
        "            layers.Input(shape=(28, 28, 1)),\n",
        "            \n",
        "            # TODO: A√±ade una capa Conv2D -> 16, (3, 3), activation='relu', padding='same', strides=2\n",
        "            layers.Conv2D(16, (3, 3), activation='relu', padding='same', strides=2),\n",
        "            \n",
        "            # TODO: A√±ade una capa Conv2D -> 8, (3, 3), activation='relu', padding='same', strides=2\n",
        "            layers.Conv2D(8, (3, 3), activation='relu', padding='same', strides=2)\n",
        "        \n",
        "        ])\n",
        "\n",
        "        # Creamos el decoder convolucional\n",
        "        self.decoder = tf.keras.Sequential([\n",
        "\n",
        "            # TODO: A√±ade una capa Conv2DTranspose -> 8, (3, 3), activation='relu', padding='same', strides=2\n",
        "            layers.Conv2DTranspose(8, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
        "            \n",
        "            # TODO: A√±ade una capa Conv2DTranspose -> 8, (3, 3), activation='relu', padding='same', strides=2\n",
        "            layers.Conv2DTranspose(16, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
        "            \n",
        "            # TODO: A√±ade una capa Conv2D -> 1, (3, 3), activation='sigmoid', padding='same'\n",
        "            layers.Conv2D(1, kernel_size=(3, 3), activation='sigmoid', padding='same')\n",
        "        \n",
        "        ])\n",
        "\n",
        "    def call(self, x):\n",
        "        # Esta funci√≥n nos permite invocar los sub modelos\n",
        "        # para poder hacer inferencia sobre cada uno (predict)\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        \n",
        "        return decoded"
      ],
      "metadata": {
        "id": "stCt0xNzzk_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lecturas recomendadas:**\n",
        "\n",
        "- [Convolution, Padding, Stride, and Pooling in CNN](https://medium.com/analytics-vidhya/convolution-padding-stride-and-pooling-in-cnn-13dc1f3ada26)\n",
        "- [Why do we need conv2d_transpose?\n",
        "](https://medium.com/@vaibhavshukla182/why-do-we-need-conv2d-transpose-2534cd2a4d98)\n",
        "- [Deconvolution](https://vincmazet.github.io/bip/restoration/deconvolution.html)\n",
        "- [Image Segmentation using deconvolution layer in Tensorflow](https://cv-tricks.com/image-segmentation/transpose-convolution-in-tensorflow/)"
      ],
      "metadata": {
        "id": "l2OVuOuwHqv0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aplicaciones de los autoencoders convolucionales en el denoising de im√°genes\n",
        "\n",
        "Para poder explorar un ejemplo de aplicaci√≥n de autoencoders donde los utilicemos para quitar el ruido de alunas im√°genes, volveremos a utilizar el dataset de modas y le agregaremos algo de ruido a las im√°genes. De este modo, entrenaremos a la red para que aprenda c√≥mo debe verse una imagen a partir de una con ruido.\n",
        "\n",
        "Cargaremos y preprocesaremos nuevamente los datos."
      ],
      "metadata": {
        "id": "aMBsYpdK6cIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, _), (x_test, _) = fashion_mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "x_train = x_train[..., tf.newaxis]\n",
        "x_test = x_test[..., tf.newaxis]\n",
        "\n",
        "print(x_train.shape)"
      ],
      "metadata": {
        "id": "erEOQdn0C5x1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agregamos ruido a los datos."
      ],
      "metadata": {
        "id": "yepxPRZ0DEPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noise_factor = 0.2\n",
        "x_train_noisy = x_train + noise_factor * tf.random.normal(shape=x_train.shape) \n",
        "x_test_noisy = x_test + noise_factor * tf.random.normal(shape=x_test.shape) \n",
        "\n",
        "x_train_noisy = tf.clip_by_value(x_train_noisy, clip_value_min=0., clip_value_max=1.)\n",
        "x_test_noisy = tf.clip_by_value(x_test_noisy, clip_value_min=0., clip_value_max=1.)"
      ],
      "metadata": {
        "id": "NyGaNXfnDDfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos explorar c√≥mo se ven los datos con algo de ruido."
      ],
      "metadata": {
        "id": "IaWoRWDsDkou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 10\n",
        "plt.figure(figsize=(20, 2))\n",
        "for i in range(n):\n",
        "    ax = plt.subplot(1, n, i + 1)\n",
        "    plt.title(\"Original + Ruido\")\n",
        "    plt.imshow(tf.squeeze(x_test_noisy[i]))\n",
        "    plt.gray()\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "plt.grid(False)"
      ],
      "metadata": {
        "id": "31WxIj_jDkQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instanciaremos, compilaremos y entrenaremos un nuevo autoencoder llamado \"denoiser\"."
      ],
      "metadata": {
        "id": "vziHzY1CDc7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "denoiser = Denoise()"
      ],
      "metadata": {
        "id": "quxg9yeHAdR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "denoiser.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "history = denoiser.fit(x_train_noisy, x_train,\n",
        "                       epochs=10,\n",
        "                       shuffle=True,\n",
        "                       validation_data=(x_test_noisy, x_test))"
      ],
      "metadata": {
        "id": "38IRvJY17Ivb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-v0_8')\n",
        "\n",
        "x = np.arange(len(history.history['loss']))\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(x, loss, label='Training')\n",
        "plt.plot(x, val_loss, label='Validation')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.savefig('loss_denoiser.png', dpi=300)"
      ],
      "metadata": {
        "id": "oMRtH2wTFMPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos explorar los detalles de ambos submodelos."
      ],
      "metadata": {
        "id": "USi-p3n5Dsvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "denoiser.encoder.summary()"
      ],
      "metadata": {
        "id": "E8UXlSb0DiBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "denoiser.decoder.summary()"
      ],
      "metadata": {
        "id": "5LKiHWpSDv_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aplicamos la inferencia sobre las im√°genes ruidosas utilizando ambos modelos."
      ],
      "metadata": {
        "id": "KRJUL90BD0Qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_imgs = denoiser.encoder(x_test_noisy).numpy()\n",
        "decoded_imgs = denoiser.decoder(encoded_imgs).numpy()"
      ],
      "metadata": {
        "id": "GzQ4GoZFDxr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 10\n",
        "plt.figure(figsize=(20, 6))\n",
        "\n",
        "\n",
        "for i in range(n):\n",
        "\n",
        "    # Mostramos original + ruido\n",
        "    ax = plt.subplot(3, n, i + 1)\n",
        "    plt.title(\"Original + Ruido\")\n",
        "    plt.imshow(tf.squeeze(x_test_noisy[i + 121]))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Mostramos reconstrucci√≥n\n",
        "    bx = plt.subplot(3, n, i + n + 1)\n",
        "    plt.title(\"Reconstruida\")\n",
        "    plt.imshow(tf.squeeze(decoded_imgs[i + 121]))\n",
        "    plt.gray()\n",
        "    bx.get_xaxis().set_visible(False)\n",
        "    bx.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Mostramos original\n",
        "    cx = plt.subplot(3, n, i + n + n + 1)\n",
        "    plt.title(\"Original\")\n",
        "    plt.imshow(tf.squeeze(x_test[i + 121]))\n",
        "    plt.gray()\n",
        "    cx.get_xaxis().set_visible(False)\n",
        "    cx.get_yaxis().set_visible(False)\n",
        "\n",
        "plt.savefig('imagen_sin_ruido.png', dpi=300)"
      ],
      "metadata": {
        "id": "SdIqgkt1Dzvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reto:** ¬øPuedes mejorar a√∫n m√°s el modelo?\n",
        "\n",
        "Te recomiendo explorar lo siguiente:\n",
        "- Modifica el n√∫mero de capas y par√°metros de convoluci√≥n por capa.\n",
        "- Modifica el n√∫mero de √©pocas de entrenamiento.\n",
        "- Puedes explorar el remover ruido de im√°genes con m√°s canales (im√°genes RGB) utilizando otros datasets."
      ],
      "metadata": {
        "id": "8U1tdrMYlDen"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trabajos relacionados y avances recientes\n",
        "\n",
        "\n",
        "Ha habido varios trabajos de investigaci√≥n y avances recientes que han contribuido al desarrollo de nuevas arquitecturas, t√©cnicas de entrenamiento mejoradas y aplicaciones emergentes.\n",
        "\n",
        "- **UNet:** Es ampliamente utilizada en el campo de la segmentaci√≥n de im√°genes, pero tambi√©n se ha aplicado con √©xito en tareas de denoising.\n",
        "- **Variational Autoencoders (VAEs):** Los VAEs son una variante de los autoencoders que se utilizan para el aprendizaje de distribuciones latentes. Han demostrado ser efectivos en el denoising de im√°genes al aprender representaciones latentes que siguen una distribuci√≥n probabil√≠stica, lo que permite una generaci√≥n m√°s controlada y realista de im√°genes limpias.\n",
        "- **GANs y Autoencoders Generativos (GANs-AE):** La combinaci√≥n de las GANs y los AE ha llevado al desarrollo de los GANs-AE. Estos modelos aprovechan la capacidad de los GANs para generar im√°genes realistas y los autoencoders para aprender representaciones latentes eficientes. Los GANs-AE han demostrado ser efectivos en el denoising y la generaci√≥n de im√°genes de alta calidad.\n",
        "\n",
        "\n",
        "#### **Tareas en el campo de la visi√≥n artificial**\n",
        "\n",
        "1. **Clasificaci√≥n de im√°genes:** La tarea de clasificaci√≥n de im√°genes implica asignar una etiqueta o categor√≠a a una imagen de entrada. Esto implica entrenar un modelo para reconocer y distinguir diferentes objetos, personas o escenas en una imagen.\n",
        "2. **Detecci√≥n de objetos:** La detecci√≥n de objetos implica localizar y clasificar m√∫ltiples objetos en una imagen. El objetivo es detectar la presencia y la ubicaci√≥n de objetos espec√≠ficos en una escena, a menudo utilizando cuadros delimitadores para delinear las regiones donde se encuentran los objetos.\n",
        "3. **_Denoising_ o reconstrucci√≥n de im√°genes:** Consiste en eliminar o reducir el ruido presente en una imagen, obteniendo una versi√≥n m√°s limpia y clara. Esta tarea es relevante en √°reas como la fotograf√≠a, la medicina y la seguridad..\n",
        "4. **Segmentaci√≥n sem√°ntica:** La segmentaci√≥n sem√°ntica implica asignar una etiqueta a cada p√≠xel de una imagen para identificar y delimitar las diferentes regiones o objetos presentes. El objetivo es comprender la estructura y el contenido de una imagen a nivel de p√≠xel.\n",
        "5. **Detecci√≥n de rostros:** La detecci√≥n de rostros es una tarea espec√≠fica de la visi√≥n artificial que implica detectar y localizar los rostros en una imagen. Es ampliamente utilizado en aplicaciones de reconocimiento facial, an√°lisis de emociones y sistemas de seguridad.\n",
        "6. **Reconocimiento y verificaci√≥n facial:** El reconocimiento facial se refiere a la tarea de identificar y reconocer a una persona espec√≠fica a partir de una imagen o secuencia de im√°genes. La verificaci√≥n facial se enfoca en verificar si una imagen de rostro coincide con una identidad espec√≠fica.\n",
        "7. **Estimaci√≥n de pose:** La estimaci√≥n de pose se refiere a la tarea de determinar la posici√≥n y orientaci√≥n de un objeto o persona en una imagen. Esto implica detectar y rastrear las articulaciones o puntos clave en una imagen para comprender la postura y el movimiento.\n",
        "8. **Estimaci√≥n de profundidad:** La estimaci√≥n de profundidad implica inferir la informaci√≥n de la distancia o la profundidad de los objetos en una imagen. Es √∫til en aplicaciones de realidad virtual, conducci√≥n aut√≥noma y sistemas de navegaci√≥n.\n",
        "9. **Super-resoluci√≥n:** La super-resoluci√≥n se refiere a aumentar la resoluci√≥n o la calidad de una imagen de baja resoluci√≥n. El objetivo es generar una versi√≥n de alta resoluci√≥n que capture m√°s detalles y claridad.\n"
      ],
      "metadata": {
        "id": "JP3NGkSg6fO2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------\n",
        "\n",
        "> Contenido creado por **Rodolfo Ferro**, 2023. <br>\n",
        "> Puedes contactarme a trav√©s de Insta ([@rodo_ferro](https://www.instagram.com/rodo_ferro/)) o Twitter ([@rodo_ferro](https://twitter.com/rodo_ferro))."
      ],
      "metadata": {
        "id": "hSdbQU3e6-Ky"
      }
    }
  ]
}